import time
import datetime
import os
import numpy as np
import torch
from dataset import COCO128Dataset
from model import SimpleYOLO, save_inference_sample
from loss_function import SimpleComputeLoss


def train_professional(resume=None, data_root='coco2017'):
    """
    ä¸“ä¸šè®­ç»ƒå‡½æ•°
    
    Args:
        resume (str, optional): æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œç”¨äºç»§ç»­è®­ç»ƒã€‚
                                å¯ä»¥æ˜¯ .pt æ–‡ä»¶è·¯å¾„ï¼Œå¦‚ 'runs/2025-12-02_23-16-21/weights/epoch_100.pt'
                                å¦‚æœä¸º Noneï¼Œåˆ™ä»å¤´å¼€å§‹è®­ç»ƒã€‚
        data_root (str): æ•°æ®é›†æ ¹ç›®å½•ï¼Œé»˜è®¤ä¸º 'coco2017'ã€‚
                         ç›®å½•ä¸‹åº”åŒ…å« images/train2017, images/val2017 ç­‰å­ç›®å½•ã€‚
    """
    # --- 1. å®éªŒç¯å¢ƒè®¾ç½® ---
    # ç”Ÿæˆ runs/2023-10-27_10-30-00 è¿™æ ·çš„ç›®å½•
    start_time = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    save_dir = os.path.join('runs', start_time)
    weights_dir = os.path.join(save_dir, 'weights')
    vis_dir = os.path.join(save_dir, 'visualizations')
    
    os.makedirs(weights_dir, exist_ok=True)
    os.makedirs(vis_dir, exist_ok=True)
    
    if resume:
        print(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {resume}")
        print(f"ğŸ“ æ–°æ—¥å¿—ç›®å½•: {save_dir}")
    else:
        print(f"ğŸš€ ä»å¤´å¼€å§‹è®­ç»ƒï¼æ—¥å¿—ç›®å½•: {save_dir}")
    
    print(f"ğŸ“‚ æ•°æ®é›†æ ¹ç›®å½•: {data_root}")

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # --- 2. æ•°æ®ä¸æ¨¡å‹ ---
    # ä½¿ç”¨çŸ©å½¢è¾“å…¥: 800x640 (å®½xé«˜)
    # æ³¨æ„: Tensor å½¢çŠ¶å°†æ˜¯ [Batch, 3, 640, 800] (Channels, Height, Width)
    train_dataset = COCO128Dataset(data_root, img_size=(800, 640), split='train')
    val_dataset = COCO128Dataset(data_root, img_size=(800, 640), split='val')
    
    train_loader = torch.utils.data.DataLoader(
        train_dataset, 
        batch_size=64, 
        shuffle=True, 
        collate_fn=COCO128Dataset.collate_fn
    )
    
    val_loader = torch.utils.data.DataLoader(
        val_dataset, 
        batch_size=8, 
        shuffle=False, 
        collate_fn=COCO128Dataset.collate_fn
    )
    
    model = SimpleYOLO(num_classes=80).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    compute_loss = SimpleComputeLoss() # å‡è®¾ä½ å·²ç»å®šä¹‰äº†è¿™ä¸ªç±»
    
    # --- 3. åŠ è½½æ£€æŸ¥ç‚¹ (å¦‚æœæä¾›) ---
    start_epoch = 0
    if resume and os.path.isfile(resume):
        print(f"ğŸ“¥ æ­£åœ¨åŠ è½½æ£€æŸ¥ç‚¹: {resume}")
        checkpoint = torch.load(resume, map_location=device)
        
        # åŠ è½½æ¨¡å‹æƒé‡
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            # å®Œæ•´æ£€æŸ¥ç‚¹æ ¼å¼ (åŒ…å« optimizer, epoch ç­‰)
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            start_epoch = checkpoint.get('epoch', 0)
            print(f"âœ… å·²åŠ è½½æ¨¡å‹ã€ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œä» Epoch {start_epoch + 1} ç»§ç»­è®­ç»ƒ")
        else:
            # ä»…åŒ…å«æ¨¡å‹æƒé‡
            model.load_state_dict(checkpoint)
            print(f"âœ… å·²åŠ è½½æ¨¡å‹æƒé‡ (ä»…æƒé‡æ–‡ä»¶)")
            # å°è¯•ä»æ–‡ä»¶åæ¨æ–­ epoch
            if 'epoch_' in os.path.basename(resume):
                try:
                    start_epoch = int(os.path.basename(resume).split('epoch_')[1].split('.')[0])
                    print(f"ğŸ“ ä»æ–‡ä»¶åæ¨æ–­èµ·å§‹ Epoch: {start_epoch}")
                except:
                    pass
    elif resume:
        print(f"âš ï¸  è­¦å‘Š: æ‰¾ä¸åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶ {resume}ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
    
    # --- 4. è®­ç»ƒå¾ªç¯ ---
    TOTAL_EPOCHS = 400 # è·‘ 400 ä¸ª epoch æ•ˆæœæ¯”è¾ƒæ˜æ˜¾
    
    model.train()
    for epoch in range(start_epoch, TOTAL_EPOCHS):
        train_loss = 0
        
        # --- A. è®­ç»ƒé˜¶æ®µ ---
        for i, (imgs, targets) in enumerate(train_loader):
            imgs = imgs.to(device)
            targets = targets.to(device)
            
            # Forward
            preds = model(imgs)
            loss = compute_loss(preds, targets)
            
            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        avg_train_loss = train_loss / len(train_loader)
        
        # --- B. éªŒè¯é˜¶æ®µ (æ–°å¢) ---
        # æ³¨æ„: ä¿æŒ model.training=True ä»¥è·å–åŸå§‹ç‰¹å¾å›¾ç”¨äº Loss è®¡ç®—
        # ä½†ä½¿ç”¨ torch.no_grad() æ¥ç¦ç”¨æ¢¯åº¦è®¡ç®—
        val_loss = 0
        with torch.no_grad():
            for i, (imgs, targets) in enumerate(val_loader):
                imgs = imgs.to(device)
                targets = targets.to(device)
                
                # Forward (æ¨¡å‹ä»åœ¨ training modeï¼Œè¿”å›åŸå§‹ç‰¹å¾å›¾)
                preds = model(imgs)
                loss = compute_loss(preds, targets)
                
                val_loss += loss.item()
        
        avg_val_loss = val_loss / len(val_loader)
        
        print(f"Epoch {epoch+1}/{TOTAL_EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}")
        
        # --- C. å¯è§†åŒ–é˜¶æ®µ (æ¯ 10 ä¸ª epoch) ---
        # å¦å¤–ï¼Œepoch 0 ä¹Ÿè·‘ä¸€ä¸‹ï¼Œçœ‹çœ‹åˆå§‹çš„ççŒœæ˜¯ä»€ä¹ˆæ ·çš„
        if epoch == 0 or (epoch + 1) % 10 == 0:
            print(f"ğŸ” æ­£åœ¨ç”Ÿæˆ Epoch {epoch+1} çš„å¯è§†åŒ–ç»“æœ...")
            save_inference_sample(model, train_dataset, epoch+1, vis_dir, prefix='train', num_samples=10)
            save_inference_sample(model, val_dataset, epoch+1, vis_dir, prefix='val', num_samples=10)
            
            # --- D. ä¿å­˜æ¨¡å‹æƒé‡ ---
            # ä¿å­˜å®Œæ•´æ£€æŸ¥ç‚¹ (åŒ…å«æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€epoch ä¿¡æ¯)
            ckpt_path = os.path.join(weights_dir, f'epoch_{epoch+1}.pt')
            checkpoint = {
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
            }
            torch.save(checkpoint, ckpt_path)
            torch.save(checkpoint, os.path.join(weights_dir, 'last.pt'))

    print(f"âœ… è®­ç»ƒç»“æŸã€‚æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {save_dir}")
    # === æ–°å¢è¿™ä¸€è¡Œ ===
    generate_evolution_gallery(save_dir, num_samples=10)

    return model


def generate_evolution_gallery(save_dir, num_samples=10):
    """
    ç”Ÿæˆ Markdown æ–‡ä»¶ï¼Œåˆ†åˆ«å±•ç¤ºè®­ç»ƒé›†å’ŒéªŒè¯é›†æ ·æœ¬çš„è¿›åŒ–è¿‡ç¨‹ã€‚
    æ¯ä¸ªæ–‡ä»¶ä¸“æ³¨äºå±•ç¤ºã€åŒä¸€å¼ å›¾ç‰‡ã€‘åœ¨ä¸åŒ Epoch çš„å˜åŒ–è¿‡ç¨‹ã€‚
    """
    vis_dir = os.path.join(save_dir, 'visualizations')
    if not os.path.exists(vis_dir):
        print("æœªæ‰¾åˆ°å¯è§†åŒ–ç›®å½•ï¼Œè·³è¿‡ç”Ÿæˆç”»å»Šã€‚")
        return

    # 1. è·å–æ‰€æœ‰ epoch æ–‡ä»¶å¤¹å¹¶æ’åº
    # æ¯”å¦‚: epoch_1, epoch_10, epoch_20...
    folders = [f for f in os.listdir(vis_dir) if f.startswith('epoch_') and os.path.isdir(os.path.join(vis_dir, f))]
    folders.sort(key=lambda x: int(x.split('_')[1]))

    if not folders:
        print("å¯è§†åŒ–ç›®å½•ä¸ºç©ºã€‚")
        return

    print(f"ğŸ¨ æ­£åœ¨ç”Ÿæˆæ¼”åŒ–ç”»å»Š (å…± {num_samples} ä¸ªæ ·æœ¬ï¼ŒTrain & Val å„ä¸€å¥—)...")

    # 2. ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ†åˆ«ç”Ÿæˆç”»å»Š
    for prefix in ['train', 'val']:
        dataset_name = 'è®­ç»ƒé›†' if prefix == 'train' else 'éªŒè¯é›†'
        
        # ä¸ºæ¯ä¸ªæ ·æœ¬ç´¢å¼• (0~9) ç”Ÿæˆä¸€ä¸ªç‹¬ç«‹çš„ MD æ–‡ä»¶
        for i in range(num_samples):
            md_filename = f'evolution_{prefix}_sample_{i}.md'
            md_path = os.path.join(save_dir, md_filename)
            
            with open(md_path, 'w', encoding='utf-8') as f:
                f.write(f"# ğŸ§¬ {dataset_name}æ ·æœ¬ {i} çš„è¿›åŒ–å²\n\n")
                f.write(f"**è§‚å¯Ÿå¯¹è±¡**: {dataset_name}ä¸­çš„ç¬¬ {i} å¼ å›¾ç‰‡\n\n")
                f.write(f"**è¯´æ˜**: å‘ä¸‹æ»šåŠ¨æŸ¥çœ‹è¯¥å›¾ç‰‡ä» Epoch {folders[0].split('_')[1]} åˆ°æœ€åçš„è®­ç»ƒå˜åŒ–ã€‚\n\n")
                f.write("---\n\n")

                # éå†æ‰€æœ‰ epoch æ–‡ä»¶å¤¹
                for folder in folders:
                    epoch_num = folder.split('_')[1]
                    img_name = f"{prefix}_img_{i}.jpg"
                    
                    # ç›¸å¯¹è·¯å¾„ (ç”¨äº Markdown æ˜¾ç¤º)
                    # ç»“æ„: visualizations/epoch_X/{prefix}_img_i.jpg
                    img_rel_path = f"visualizations/{folder}/{img_name}"
                    
                    # ç»å¯¹è·¯å¾„ (ç”¨äºæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨)
                    full_path = os.path.join(vis_dir, folder, img_name)
                    
                    if os.path.exists(full_path):
                        f.write(f"## Epoch {epoch_num}\n")
                        f.write(f"![Epoch {epoch_num}]({img_rel_path})\n\n")
                    else:
                        # å¦‚æœæŸä¸ª epoch æ²¡ç”Ÿæˆè¿™å¼ å›¾ (æå°‘è§)
                        f.write(f"## Epoch {epoch_num}\n")
                        f.write(f"> *è¯¥ Epoch ç¼ºå¤±å›¾ç‰‡*\n\n")

    print(f"âœ… ç”»å»Šç”Ÿæˆå®Œæ¯•ï¼è¯·åœ¨ VS Code ä¸­æ‰“å¼€ '{save_dir}/evolution_train_sample_X.md' å’Œ 'evolution_val_sample_X.md' æŸ¥çœ‹ã€‚")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='YOLO è®­ç»ƒè„šæœ¬')
    parser.add_argument('--resume', type=str, default=None,
                        help='æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œç”¨äºç»§ç»­è®­ç»ƒã€‚ä¾‹å¦‚: runs/2025-12-02_23-16-21/weights/epoch_100.pt')
    parser.add_argument('--data', type=str, default='coco2017',
                        help='æ•°æ®é›†æ ¹ç›®å½•ã€‚é»˜è®¤: coco2017')
    args = parser.parse_args()
    
    # ç¡®ä¿ä¹‹å‰çš„ SimpleYOLO, COCO128Dataset, SimpleComputeLoss, non_max_suppression éƒ½åœ¨ä¸Šä¸‹æ–‡ä¸­
    train_professional(resume=args.resume, data_root=args.data)